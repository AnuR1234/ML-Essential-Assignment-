{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team Members\n",
    "### Anu Reddy - anu.reddy@stud.uni-heidelberg.de\n",
    "### Keerthan Ugrani - keerthan.ugrani@stud.uni-heidelberg.de\n",
    "### Florain Tichawa - florian.tichawa@stud.uni-heidelberg.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Hand-Crafted Network\n",
    "### Task 1.1: Neuron Definition\n",
    "First, we create a Neuron class taking a bias, list of weights, and activation function as a base.\n",
    "To demonstrate its functionality, we use the weighted average as default activation function, and run it with a simple test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def weighted_avg(values, params):\n",
    "        '''\n",
    "        The function weighted_avg calculates the weighted average of input values and parameters.\n",
    "        Input:\n",
    "        values: The input values, a numpy array\n",
    "        params: The parameters of the neuron, a numpy array\n",
    "        Output:\n",
    "        The weighted average of input values and parameters\n",
    "        '''\n",
    "        return np.mean(values * params)\n",
    "    \n",
    "    def __init__(self, bias=0.0, params=list(), afunc=weighted_avg):\n",
    "        self.bias = bias\n",
    "        self.params = np.asarray(params)\n",
    "        self.afunc = afunc\n",
    "    \n",
    "    def activate(self, inputs):\n",
    "        '''\n",
    "        The function activate calculates the output of the neuron for the given input values.\n",
    "        Input:\n",
    "        inputs: The input values, a numpy array\n",
    "        Output:\n",
    "        The output of the neuron\n",
    "        '''\n",
    "        return self.bias + self.afunc(inputs, self.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(bias=0.5, params=[0.1, 0.2, 0.3])\n",
    "assert neuron.activate([3, 2, 1]) == 0.5 + (3 * 0.1 + 2 * 0.2 + 1 * 0.3) / 3\n",
    "print(\"weighted_avg of\", [3, 2, 1], \"with weights\", neuron.params, \"and bias\", neuron.bias, \"is\", neuron.activate([3, 2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our three activation functions <code>logical_or</code>, <code>masked_or</code>, and <code>xnor</code> (\"Perfect match\") to use in our neuron.\n",
    "For <code>logical_or</code>, we only need the input data. For <code>masked_or</code> and <code>xnor</code>, we use the neuron parameters as bitmasks.\n",
    "\n",
    "To verify our implementation, we run a few simple test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_or(values, params):\n",
    "    '''\n",
    "    This function performs a logical OR operation on the values.\n",
    "    Input:\n",
    "    values: numpy array\n",
    "    params: not used in this function\n",
    "        This parameter is not used in this function but is included to maintain a consistent function signature.\n",
    "    Output:\n",
    "    bool:The result of applying the logical OR operation to the input values.\n",
    "    '''\n",
    "    return values.any()\n",
    "\n",
    "def masked_or(values, params):\n",
    "    '''\n",
    "    This function performs a logical AND operation on each corresponding pair of elements in 'values' and 'params',\n",
    "    then returns the result of applying a logical OR operation to the results of the AND operations.\n",
    "    Input:\n",
    "    values: numpy array\n",
    "    params: numpy array\n",
    "    Output:\n",
    "    bool:The result of applying the masked OR operation to the input values and parameters.\n",
    "    '''\n",
    "    return np.logical_and(values, params).any()\n",
    "\n",
    "def xnor(values, params):\n",
    "    '''\n",
    "    This function performs a logical XNOR operation on each appropriate pair of entries in the 'values' and 'params' arrays.\n",
    "    If the number of 'True' inputs is even, the XNOR operation returns 'True', else it returns 'False'. \n",
    "    The inverse of the XOR operator, which returns 'True' if the number of 'True' inputs is odd.\n",
    "    Input:\n",
    "    values: numpy array   \n",
    "    params: numpy array \n",
    "    Output:\n",
    "    bool:The result of applying the XNOR operation to the input values and parameters.\n",
    "    '''\n",
    "    return not np.logical_xor(values, params).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = np.array([0, 0, 0])\n",
    "list2 = np.array([0, 0, 1])\n",
    "list3 = np.array([0, 1, 1])\n",
    "\n",
    "assert logical_or(list1, list()) == 0\n",
    "print(\"logical_or on\", list1, \"is\", logical_or(list1, list()))\n",
    "assert logical_or(list2, list()) == 1\n",
    "print(\"logical_or on\", list2, \"is\", logical_or(list2, list()))\n",
    "assert logical_or(list3, list()) == 1\n",
    "print(\"logical_or on\", list3, \"is\", logical_or(list3, list()))\n",
    "\n",
    "assert masked_or(list2, list1) == 0\n",
    "print(\"masked_or on\", list2, \"with mask\", list1, \"is\", masked_or(list2, list1))\n",
    "assert masked_or(list2, list3) == 1\n",
    "print(\"masked_or on\", list2, \"with mask\", list3, \"is\", masked_or(list2, list3))\n",
    "\n",
    "assert xnor(list2, list3) == 0\n",
    "print(\"xnor on\", list2, \"and\", list1, \"is\", xnor(list2, list3))\n",
    "assert xnor(list3, list3) == 1\n",
    "print(\"xnor on\", list3, \"and\", list3, \"is\", xnor(list3, list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn these activation functions into proper neurons, we now have to create a couple of neurons using these methods.\n",
    "To demonstrate their functionality, we will run the same test cases on these neurons.\n",
    "For these simple cases, the bias is always zero, and the output is only determined by the input (And the mask supplied through the <code>params</code> attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_neuron = Neuron(afunc=logical_or)\n",
    "assert or_neuron.activate(list1) == 0\n",
    "print(\"logical_or neuron on\", list1, \"is\", or_neuron.activate(list1))\n",
    "assert or_neuron.activate(list2) == 1\n",
    "print(\"logical_or neuron on\", list2, \"is\", or_neuron.activate(list2))\n",
    "assert or_neuron.activate(list3) == 1\n",
    "print(\"logical_or neuron on\", list3, \"is\", or_neuron.activate(list3))\n",
    "\n",
    "masked_neuron_list1 = Neuron(params=list1, afunc=masked_or)\n",
    "masked_neuron_list3 = Neuron(params=list3, afunc=masked_or)\n",
    "assert masked_neuron_list1.activate(list2) == 0\n",
    "print(\"masked_or neuron on\", list2, \"with mask\", list1, \"is\", masked_neuron_list1.activate(list2))\n",
    "assert masked_neuron_list3.activate(list2) == 1\n",
    "print(\"masked_or neuron on\", list2, \"with mask\", list3, \"is\", masked_neuron_list3.activate(list2))\n",
    "\n",
    "xnor_neuron = Neuron(params=list3, afunc=xnor)\n",
    "assert xnor_neuron.activate(list2) == 0\n",
    "print(\"xnor neuron on\", list2, \"and\", list1, \"is\", xnor_neuron.activate(list2))\n",
    "assert xnor_neuron.activate(list3) == 1\n",
    "print(\"xnor neuron on\", list3, \"and\", list3, \"is\", xnor_neuron.activate(list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Network Creation\n",
    "The easiest way to model a hypercube of dimension <code>M</code> within one layer of neural network is to create one neuron per dimension, and using the neuron output (0 or 1) as coordinates in their respective dimension.\n",
    "To achieve that, every neuron in the first layer has to stricly map to a certain region of the input parameter space, ideally covering the entire parameter space.\n",
    "One solution would be to define a triangle region for each parameter using three line equations defined by <code>m</code> (Slope of the line), <code>t</code> (Offset from zero), and <code>s</code> (Sign of the inequality to differentiate between the two sides of the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_coords_in_triangle(coords, triangles):\n",
    "    ''' \n",
    "    The function `are_coords_in_triangle` determines whether given coordinates fall within a specified triangle.\n",
    "    Inputs:\n",
    "    coords: A 2-element list, tuple, or array representing the x and y coordinates of a point.\n",
    "    triangles:A nine-element list, tuple, or array containing three lines forming a triangle. \n",
    "              Each line is defined by three parameters: slope, y-intercept, and a boolean indicating whether the region above the line (True) or the region below the line (False) is inside the triangle.\n",
    "    Output:\n",
    "    Returns: A boolean showing whether the coordinates provided fall within the triangle. If True, the coordinates are within the triangle; else, they are outside.\n",
    "    '''\n",
    "    match = list()\n",
    "\n",
    "    for i in range(3):\n",
    "        above = triangles[3 * i + 2]\n",
    "        line = lambda x: triangles[3 * i] * x + triangles[3 * i + 1]\n",
    "\n",
    "        if above:\n",
    "            match.append(coords[1] > line(coords[0]))\n",
    "        else:\n",
    "            match.append(coords[1] <= line(coords[0]))\n",
    "\n",
    "    return np.all(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the implementation of the triangle function, we use a simple test case of f<sub>1</sub>(x)= 0, f<sub>2</sub>(x)= x+0, and f<sub>3</sub>(x)= -x+2, which should generate a triangle with corners (0, 0), (1, 1), and (2, 0).\n",
    "As shown by the coloring of the three corners, points exactly on the edges of the triangle are only considered to be inside the triangle if the line they're on is defined with <code>s = False<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle = [0, 0, True, 1, 0, False, -1, 2, False]\n",
    "\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(-0.5, 2.5, 300), np.linspace(-1, 2, 300))\n",
    "grid_points = np.stack([grid_x.ravel(), grid_y.ravel()], axis=1)\n",
    "grid_image = [are_coords_in_triangle(coords, triangle) for coords in grid_points]\n",
    "\n",
    "plt.imshow(np.reshape(grid_image, (300, 300)), extent=[-0.5, 2.5, -1, 2], origin=\"lower\", cmap=\"binary\", alpha=0.5)\n",
    "plt.scatter(0, 0, marker='x', s=100, color=\"black\" if are_coords_in_triangle((0, 0), triangle) else \"red\")\n",
    "plt.scatter(1, 1, marker='x', s=100, color=\"black\" if are_coords_in_triangle((1, 1), triangle) else \"red\")\n",
    "plt.scatter(2, 0, marker='x', s=100, color=\"black\" if are_coords_in_triangle((2, 0), triangle) else \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to determine the decision boundaries for the sample data given by tiling the feature space.\n",
    "To simplify the process, the feature space will have a range of \\[0, 1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [\n",
    "    [0.1, 0.1, \"green\", \"o\"],\n",
    "    [0.25, 0.15, \"green\", \"o\"],\n",
    "    [0.17, 0.2, \"blue\", \"+\"],\n",
    "    [0.25, 0.4, \"blue\", \"+\"],\n",
    "    [0.12, 0.7, \"blue\", \"+\"],\n",
    "    [0.25, 0.98, \"red\", \"_\"],\n",
    "    [0.27, 0.97, \"red\", \"_\"],\n",
    "    [0.29, 0.95, \"red\", \"_\"],\n",
    "    [0.3, 0.99, \"red\", \"_\"],\n",
    "    [0.4, 0.4, \"green\", \"o\"],\n",
    "    [0.5, 0.5, \"green\", \"o\"],\n",
    "    [0.42, 0.55, \"green\", \"o\"],\n",
    "    [0.48, 0.87, \"green\", \"o\"],\n",
    "    [0.7, 0.8, \"green\", \"o\"],\n",
    "    [0.75, 0.95, \"green\", \"o\"],\n",
    "    [0.85, 0.85, \"green\", \"o\"],\n",
    "    [0.88, 0.9, \"green\", \"o\"],\n",
    "    [0.35, 0.17, \"red\", \"_\"],\n",
    "    [0.5, 0.1, \"red\", \"_\"],\n",
    "    [0.65, 0.2, \"red\", \"_\"],\n",
    "    [0.8, 0.23, \"red\", \"_\"],\n",
    "    [0.9, 0.2, \"red\", \"_\"],\n",
    "    [0.78, 0.15, \"red\", \"_\"],\n",
    "    [0.82, 0.5, \"blue\", \"+\"],\n",
    "    [0.9, 0.4, \"blue\", \"+\"],\n",
    "    [0.95, 0.6, \"blue\", \"+\"],\n",
    "]\n",
    "\n",
    "tiles = list()\n",
    "labels = list()\n",
    "\n",
    "labels.append(\"Green\")\n",
    "tiles.append([0.0, 0.0, True, 0.33, 0.1, False, -3.0, 1.0, False])\n",
    "labels.append(\"Blue\")\n",
    "tiles.append([0.33, 0.1, True, -999, 1.0, True, -2.0, 1.0, False])\n",
    "labels.append(\"Red\")\n",
    "tiles.append([-2.0, 1.0, True, 0.0, 1.0, False, 1.0, 0.5, True])\n",
    "labels.append(\"Green\")\n",
    "tiles.append([1.0, 0.5, False, 0.0, 0.67, True, 1.0, -0.2, True])\n",
    "labels.append(\"Green\")\n",
    "tiles.append([-2.0, 1.0, True, 0.0, 0.67, False, 1.0, -0.2, True])\n",
    "labels.append(\"Blue\")\n",
    "tiles.append([1.0, -0.2, False, -1.0, 1.2, True, 999, -999, True])\n",
    "labels.append(\"Red\")\n",
    "tiles.append([-1.0, 1.2, False, 1.0, -0.2, False, 0.0, 0.19, True])\n",
    "labels.append(\"Red\")\n",
    "tiles.append([-3.0, 1.0, True, 0.0, 0.19, False, 999, -999, True])\n",
    "\n",
    "for tile, label in zip(tiles, labels):\n",
    "    grid_x, grid_y = np.meshgrid(np.linspace(0, 1, 300), np.linspace(0, 1, 300))\n",
    "    grid_points = np.stack([grid_x.ravel(), grid_y.ravel()], axis=1)\n",
    "    grid_image = [are_coords_in_triangle(coords, tile) for coords in grid_points]\n",
    "\n",
    "    plt.imshow(np.reshape(grid_image, (300, 300)), extent=[0, 1, 0, 1], cmap=label + \"s\",\n",
    "               origin=\"lower\", vmin=0, vmax=1, alpha=0.3)\n",
    "\n",
    "for point in points:\n",
    "    plt.scatter(point[0], point[1], color=point[2], marker=point[3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these regions in place, we can now turn them into our first layer of the neural network, and use the previously created binary neurons to build our second layer, and create our prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = [Neuron(afunc=are_coords_in_triangle, params=tile) for tile in tiles]\n",
    "\n",
    "red_neuron = Neuron(afunc=masked_or, params=[label == \"Red\" for label in labels])\n",
    "green_neuron = Neuron(afunc=masked_or, params=[label == \"Green\" for label in labels])\n",
    "blue_neuron = Neuron(afunc=masked_or, params=[label == \"Blue\" for label in labels])\n",
    "second_layer = [red_neuron, green_neuron, blue_neuron]\n",
    "\n",
    "layers = [first_layer, second_layer]\n",
    "\n",
    "def predict_nn(data, layers):\n",
    "    '''\n",
    "    The function predict_nn is responsible for propagating data through a given neural network.\n",
    "    Inputs:\n",
    "    data: The input data to the neural network, which is a numpy array or list.\n",
    "    layers: The list of layers in the neural network, each layer being a list of neurons.\n",
    "    Output:\n",
    "    The result of the propagation of data through the neural network, which is a list with the output of each neuron in the last layer.\n",
    "    '''\n",
    "    for layer in layers:\n",
    "        data = [neuron.activate(data) for neuron in layer]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network should be able to recreate the labels and decision regions without any errors, meaning we should be able to recreate the previous plot using the network's predictions for coloring alone, and correctly predict the label of every training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 300\n",
    "\n",
    "def rgba(rgb, a):\n",
    "    '''\n",
    "    The function rgba is responsible for converting an RGB color code to RGBA by adding an alpha (transparency) channel.\n",
    "    Input:\n",
    "    rgb: The RGB color code, which is a list of three elements.\n",
    "    a: The value for the alpha (transparency) channel.\n",
    "    Output:\n",
    "    The RGBA color code, which is a list of four elements.\n",
    "    '''\n",
    "    \n",
    "    return [rgb[0], rgb[1], rgb[2], a]\n",
    "\n",
    "def label_to_marker(label):\n",
    "    '''\n",
    "    The function label_to_marker is responsible for converting a color label (used in the context of a plot) to a marker symbol.\n",
    "    Input:\n",
    "    label: The color label, which is a list of three elements corresponding to an RGB color code.\n",
    "    Output:\n",
    "    The marker symbol, which is a string.\n",
    "    '''\n",
    "    if label == [1.0, 0.0, 0.0]:\n",
    "        return \"_\"\n",
    "    elif label == [0.0, 1.0, 0.0]:\n",
    "        return \"o\"\n",
    "    elif label == [0.0, 0.0, 1.0]:\n",
    "        return \"+\"\n",
    "    else:\n",
    "        return \"x\"\n",
    "\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(0, 1, res), np.linspace(0, 1, res))\n",
    "grid_points = np.stack([grid_x.ravel(), grid_y.ravel()], axis=1)\n",
    "grid_image = [rgba(predict_nn(coords, layers), 0.2) for coords in grid_points]\n",
    "\n",
    "plt.imshow(np.reshape(grid_image, (res, res, 4)), extent=[0, 1, 0, 1],\n",
    "           origin=\"lower\", vmin=0, vmax=1)\n",
    "\n",
    "for point in points:\n",
    "    label = predict_nn(point[0:2], layers)\n",
    "    plt.scatter(point[0], point[1], color=label, marker=label_to_marker(label))\n",
    "\n",
    "label = predict_nn([0.35, 0.2], layers)\n",
    "plt.scatter(0.35, 0.2, color=label, marker=label_to_marker(label))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated, the network is perfectly capable of reproducing the labels of the training data and the decision regions without failure.\n",
    "The only issue with the presented network is that it can be very challenging to cover the entire input parameter space with the chosen approach of tiling the space into triangles - The small region surrounding the coordinates (0.35, 0.2) (Marked with a black X on the plot), for example, does not have any neurons associated with it, resulting in a (0.0, 0.0, 0.0) prediction and a black shade in the plot.\n",
    "### Task 1.3: Generalization for higher-dimensional input\n",
    "The chosen approach heavily relies on the two-dimensionality of input data, as a higher-dimensional variant of the triangle tiling requires much more complex boundary planes instead of simple line equations, which makes it significantly harder to manually set them.\n",
    "\n",
    "A similar, but easier approach could use n-dimensional spheres instead of triangles, then the generalization for higher-dimensional input would be a lot more manageable, and could even be automated for any training data (Which would effectively result in a nearest-neighbor-like algorithm), allowing the implementation to be much more useful.\n",
    "Especially the automated implementation would yield a lot more input regions unassociated with any neurons in the first layer though, which would make it even more difficult to predict labels outside of the training set.\n",
    "\n",
    "Similarly, more complex label distributions make it significantly harder to properly fit neurons onto the training set, which results in a higher total number of neurons, and a huge decrease in runtime, because every neuron in every layer has to execute its activation function for every input data point. With the current (Relatively simple) network, the prediction of the 90k data point used to generate the plot above already took several seconds - although a more streamlined implementation making use of NumPy's array functions could probably help out with this issue, especially with the n-sphere approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  Linear Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a feed forward network the output of each layer l is calculated iteratively by <br>\n",
    "$Z_0 = X$ <br>\n",
    "$Z_l = Z_(l-1) ∙ B_l + b_l …… Eq1$ <br>\n",
    "$Z_l = ∅_1 (Z_1) …… Eq2$ <br>\n",
    "\n",
    "Assume that the activation function in each of the hidden layers of a neural network with depth L>1 is the identity function\n",
    "Let $B_1, B_2, .... B_L$ be the weights of vector of each layer and X represents the input.\n",
    "\n",
    "As the activation function in each layer is identity function then according to eq1 and eq2, we can write it as\n",
    "\n",
    "$Z_l= B_L∙(B_(L-1)∙( B_(L-2)...( B_2  ∙(B_1∙X))))$ <br>\n",
    "\n",
    "This can also be expressed as\n",
    "\n",
    "$Z_l= B_L∙ B_(L-1)∙ B_(L-1) ... B_2  ∙ B_1∙X$ <br>\n",
    "\n",
    "Consider\n",
    "\n",
    "$\\hat B_L = B_L∙ B_(L-1)∙ B_(L-1) ... B_2  ∙ B_1∙X .... Eq3$ <br>\n",
    "\n",
    "Then the eq3 can be written as\n",
    "\n",
    "$Z_l = B_L^| (X)$ <br>\n",
    "\n",
    "This holds true even for 1 layer neural network where the output of the network is a linear combination of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Programming a neural network\n",
    "In this exercise we want to implement a simple Multi-Layer Perceptron classi\u001c",
    "\n",
    "\n",
    "er using numpy. The\n",
    "python code below de\u001c",
    "\n",
    "\n",
    "nes an MLP class with ReLU activations in the hidden layers and softmax\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This LinearLayer class depicts a neural network's linear (or dense, or completely connected) layer. It transforms its inputs linearly by multiplying them by a weights matrix and adding a bias vector. Backpropagation requires the gradients for the weights and biases, as well as the downstream gradient for the preceding layer, which the backward technique computes. The gradient descent step is applied to the weights and biases using the update method using the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        The function forward is responsible for applying the Rectified Linear Unit (ReLU) operation on the input data.\n",
    "        input: The input to the layer which is a numpy array\n",
    "        output:\n",
    "        The result of applying the ReLU function on the input\n",
    "        '''\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(0,input)\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        '''\n",
    "        The function backward is responsible for computing the derivative of ReLU from the upstream gradient and the stored input.\n",
    "        Input:\n",
    "        upstream_gradient: The gradient of the loss function with respect to the output of the ReLU function\n",
    "        Output:\n",
    "        The downstream gradient for the next layer\n",
    "        '''\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        grad_relu=self.input>0\n",
    "        downstream_gradient = upstream_gradient * grad_relu\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        The function update is not applicable for the ReLU layer because it does not have any learnable parameters. \n",
    "        Hence it is implemented as a pass function.\n",
    "        Input:\n",
    "        learning_rate: The learning rate for the gradient update\n",
    "        Returns:\n",
    "        None\n",
    "        '''\n",
    "        pass # ReLU is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        The forward function applies the softmax function on the input data.\n",
    "        Input:\n",
    "        input: The input to the layer which is a numpy array\n",
    "        Output:\n",
    "        The result of applying the softmax function on the input\n",
    "        '''\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        e_x = np.exp(input - np.max(input,axis=1,keepdims=True))\n",
    "        softmax = e_x / e_x.sum(axis=1,keepdims=True)\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        '''\n",
    "        The backward function computes the loss derivative with respect to the stored inputs using cross-entropy loss \n",
    "        and the chain rule for softmax.\n",
    "        Input:\n",
    "        predicted_posteriors: The predicted probabilities for each class from the forward pass\n",
    "        true_labels: The actual labels for the data\n",
    "        Output:\n",
    "        The downstream gradient for the next layer\n",
    "        '''\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        num_units = predicted_posteriors.shape[1]\n",
    "        d_layer = np.eye(num_units)\n",
    "        downstream_gradient = np.dot(true_labels, d_layer) #chain rule\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        The function update is not applicable for the softmax layer because it does not have any learnable parameters. \n",
    "        Hence it is implemented as a pass function.\n",
    "        Input:\n",
    "        learning_rate: The learning rate for the gradient update\n",
    "        Output:\n",
    "        None\n",
    "        '''\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    # Initialization function\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # Randomly initialize weights (B) and biases (b)\n",
    "        # Here, the weights matrix B has a size of (n_inputs, n_outputs)\n",
    "        # and the biases vector b has a size of (n_outputs,)\n",
    "        self.B = np.random.normal(size=(n_inputs, n_outputs))\n",
    "        self.b = np.random.normal(size=(n_outputs,))\n",
    "\n",
    "    # Forward pass function\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the linear layer.\n",
    "        Inputs:\n",
    "        :param  A 2D numpy array where each row is a sample and each column is a feature.\n",
    "        Output:\n",
    "        :returns: A 2D numpy array where each row is a sample and each column is a preactivation.\n",
    "        \"\"\"\n",
    "        # Remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # Compute the linear transformation of input using weights and biases\n",
    "        # This is the preactivation for the subsequent non-linear layer\n",
    "        preactivations = np.dot(input, self.B) + self.b\n",
    "        return preactivations\n",
    "\n",
    "    # Backward pass function\n",
    "    def backward(self, upstream_gradient):\n",
    "        '''\n",
    "         Inputs:\n",
    "        - upstream_gradient: array of shape (batch_size, n_outputs)\n",
    "          Gradient of the loss with respect to the outputs of this layer.\n",
    "\n",
    "        Returns:\n",
    "        - downstream_gradient: array of shape (batch_size, n_inputs)\n",
    "          Gradient of the loss with respect to the inputs to this layer.\n",
    "        '''\n",
    "        # Compute the gradient of the biases\n",
    "        # It's simply the sum of upstream gradients across the batch dimension\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        # Compute the gradient of the weights\n",
    "        # It's the input transposed times the upstream gradient\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient)\n",
    "        # Compute the downstream gradient to be passed to the preceding layer\n",
    "        # It's the upstream gradient times the weights transposed\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "\n",
    "    # Update function\n",
    "    def update(self, learning_rate):\n",
    "        ''' \n",
    "          Performs a gradient descent update of the weights and biases.\n",
    "\n",
    "        Inputs:\n",
    "        - learning_rate: float, learning rate for the gradient update.\n",
    "        '''\n",
    "        # Update the weights and biases by batch gradient descent\n",
    "        # Here we subtract because we want to move opposite to the gradient for minimizing the loss\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    # Initialization function\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # Construct a multi-layer perceptron with ReLU activation in the hidden layers and softmax output\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers = []\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            # Add Linear layer followed by a ReLU layer\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # Create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    # Forward pass function\n",
    "    def forward(self, X):\n",
    "        '''The forward function is a key component of a neural network model, as it carries out the computations of the network from the input layer all the way to the output layer.\n",
    "        input:The input X is a 2-dimensional numpy array where each row corresponds to an instance in the current mini-batch of input data.\n",
    "        output:The output of the forward function is another 2-dimensional numpy array of the same shape as the input.\n",
    "        '''\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # Flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # Compute the forward pass\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    # Backward pass function\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        '''This function specifically computes the gradient of the loss function with respect to the weights and biases in the network.\n",
    "        Inputs:\n",
    "        - predicted_posteriors: It's a 2D array with shape (batch_size, n_classes).[predicted probabilities for each class for each instance in the batch.]\n",
    "        - true_classes:One-hot encoded 2d array with shape (batch_size, n_classes).[These are the true labels for each instance in the batch.]\n",
    "        '''\n",
    "        downstream_gradient = predicted_posteriors - true_classes # cross-entropy loss derivative\n",
    "        for layer in reversed(self.layers[:-1]): # Exclude last layer which is OutputLayer\n",
    "            downstream_gradient = layer.backward(downstream_gradient)\n",
    "    # Handle OutputLayer separately\n",
    "        self.layers[-1].backward(predicted_posteriors, true_classes)\n",
    "\n",
    "    # Update function\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        ''' The update function is responsible for performing one step of training, which includes forward propagation, backward propagation, and updating the weights and biases of the network.\n",
    "        Inputs:\n",
    "        - X: A batch of input data.  A numpy array\n",
    "        - Y: The true labels corresponding to the input data X. A numpy array\n",
    "        - learning_rate :This is a hyperparameter that determines the step size when updating the weights during backpropagation.\n",
    "        '''\n",
    "        # Compute the forward pass\n",
    "        posteriors = self.forward(X)\n",
    "        # Perform the backward pass\n",
    "        self.backward(posteriors, Y)\n",
    "        # Update parameters for each layer\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    # Training function\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the MLP using mini-batch gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - x: A numpy array of shape (N, D) giving training data, where N is the number of data points and D is the number of features.\n",
    "        - y: A numpy array of shape (N, C) giving training labels, where C is the number of classes. Labels should be one-hot encoded.\n",
    "        - n_epochs: An integer giving the number of training epochs (how many times the learning algorithm will work through the entire training set).\n",
    "        - batch_size: An integer giving the number of training examples per mini-batch.\n",
    "        - learning_rate: A float giving the learning rate for the optimization.\n",
    "\n",
    "        Returns: No explicit return, but the weights and biases of the MLP are updated internally.\n",
    "        \"\"\"\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # Reorder data for every epoch\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # Create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # Perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)\n",
    "    @staticmethod\n",
    "    def to_one_hot(Y, n_classes):\n",
    "        ''' The to_one_hot function is a utility function that converts class labels into a one-hot encoded representation.\n",
    "        Input:\n",
    "        Y: a 1-D numpy array of size n_samples containing class labels. Each label is an integer between 0 and n_classes-1.\n",
    "        n_classes: the number of unique classes, a scalar integer.\n",
    "        Output:\n",
    "        A 2-D numpy array of shape (n_samples, n_classes). \n",
    "        '''\n",
    "        n_samples = len(Y)\n",
    "        Y_one_hot = np.zeros((n_samples, n_classes))\n",
    "        Y_one_hot[np.arange(n_samples), Y] = 1\n",
    "        return Y_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [5, 5, n_classes]\n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "\n",
    "    # train\n",
    "    network.train(X_train, MLP.to_one_hot(Y_train, n_classes), n_epochs, batch_size, learning_rate)\n",
    "\n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate = np.mean(predicted_classes != np.argmax(MLP.to_one_hot(Y_test, n_classes), axis=1))\n",
    "    print(\"error rate:\", error_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<title>W3.CSS</title>\n",
    "<body>\n",
    "<div style=\"color: green; font-weight: bold\">Comments:(using the sample code for reference)</div> \n",
    "<p> Major differences mentioned </p>\n",
    "  <p>1. MLP class:</p>\n",
    "    <li>1. In this code after a loop over the rest of the layers, the backward method of the OutputLayer is called separately. In the sample's solution, all levels, including OutputLayer, are included in the same loop. If the backward approach of OutputLayer requires different inputs than the previous layers, this way of the implementation  may be preferable.</li>\n",
    "    <li>2.Utility function to_one_hot is implemented correctly and is a good addition for handling categorical labels.(which is a addtional method from sample )</li>\n",
    "  <p>2. Main method:</p>\n",
    "    <li>1.The main difference between the 2 codes is that this code uses static method one-hot encoding while the sample utilizes uses the labels from the 'make_moons' function due to which in training Y_train differs as here it uses one-hot encoded labels but sample directly uses the y_train from 'make_moons'</li>\n",
    "    <li>2.The change to one-hot encoding may cause the student's code to be slower than the teacher's. While this can be advantageous in some situations, it can also result in additional processing overhead. If the MLP model and error computation can function with the original labels, the one-time encoding change may not be required but it can be valid alternative approach may lead to better training results.  </li>\n",
    "    <li>3. Remaining code looks similar. </li>\n",
    "  <p>3. LinearLayer class:</p>\n",
    "    <li>1.Backward Pass (backward): The backward method in both codes computes the gradients of the biases and weights, as well as the downstream gradient to be passed to the preceding layer.</li>\n",
    "    <li>2.Update (update): Both codes perform the update of weights and biases using gradient descent. The learning rate is multiplied with the gradients of the weights and biases and subtracted from the existing weights and biases to minimize the loss.</li>\n",
    "    <li>3.Adding some error handling or input validation could make the code more resilient. For example, you could add checks to guarantee that the input to the forward method is a 2D numpy array as intended.</li>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
