{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This LinearLayer class depicts a neural network's linear (or dense, or completely connected) layer. It transforms its inputs linearly by multiplying them by a weights matrix and adding a bias vector. Backpropagation requires the gradients for the weights and biases, as well as the downstream gradient for the preceding layer, which the backward technique computes. The gradient descent step is applied to the weights and biases using the update method using the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(0,input)\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        self.input = input\n",
    "        grad_relu = input > 0\n",
    "        downstream_gradient = upstream_gradient * grad_relu\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        e_x = np.exp(input - np.max(input))\n",
    "        softmax = e_x / e_x.sum(axis=0)\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        num_units = predicted_posteriors.shape[1]\n",
    "        d_layer = np.eye(num_units)\n",
    "        downstream_gradient = np.dot(true_labels, d_layer) #chain rule\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    # Initialization function\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # Randomly initialize weights (B) and biases (b)\n",
    "        # Here, the weights matrix B has a size of (n_inputs, n_outputs)\n",
    "        # and the biases vector b has a size of (n_outputs,)\n",
    "        self.B = np.random.normal(size=(n_inputs, n_outputs))\n",
    "        self.b = np.random.normal(size=(n_outputs,))\n",
    "\n",
    "    # Forward pass function\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the linear layer.\n",
    "        Inputs:\n",
    "        :param  A 2D numpy array where each row is a sample and each column is a feature.\n",
    "        Output:\n",
    "        :returns: A 2D numpy array where each row is a sample and each column is a preactivation.\n",
    "        \"\"\"\n",
    "        # Remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # Compute the linear transformation of input using weights and biases\n",
    "        # This is the preactivation for the subsequent non-linear layer\n",
    "        preactivations = np.dot(input, self.B) + self.b\n",
    "        return preactivations\n",
    "\n",
    "    # Backward pass function\n",
    "    def backward(self, upstream_gradient):\n",
    "        '''\n",
    "         Inputs:\n",
    "        - upstream_gradient: array of shape (batch_size, n_outputs)\n",
    "          Gradient of the loss with respect to the outputs of this layer.\n",
    "\n",
    "        Returns:\n",
    "        - downstream_gradient: array of shape (batch_size, n_inputs)\n",
    "          Gradient of the loss with respect to the inputs to this layer.\n",
    "        '''\n",
    "        # Compute the gradient of the biases\n",
    "        # It's simply the sum of upstream gradients across the batch dimension\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        # Compute the gradient of the weights\n",
    "        # It's the input transposed times the upstream gradient\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient)\n",
    "        # Compute the downstream gradient to be passed to the preceding layer\n",
    "        # It's the upstream gradient times the weights transposed\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "\n",
    "    # Update function\n",
    "    def update(self, learning_rate):\n",
    "        ''' \n",
    "          Performs a gradient descent update of the weights and biases.\n",
    "\n",
    "        Inputs:\n",
    "        - learning_rate: float, learning rate for the gradient update.\n",
    "        '''\n",
    "        # Update the weights and biases by batch gradient descent\n",
    "        # Here we subtract because we want to move opposite to the gradient for minimizing the loss\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    # Initialization function\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # Construct a multi-layer perceptron with ReLU activation in the hidden layers and softmax output\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers = []\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            # Add Linear layer followed by a ReLU layer\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # Create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    # Forward pass function\n",
    "    def forward(self, X):\n",
    "        '''The forward function is a key component of a neural network model, as it carries out the computations of the network from the input layer all the way to the output layer.\n",
    "        input:The input X is a 2-dimensional numpy array where each row corresponds to an instance in the current mini-batch of input data.\n",
    "        output:The output of the forward function is another 2-dimensional numpy array of the same shape as the input.\n",
    "        '''\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # Flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # Compute the forward pass\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    # Backward pass function\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        '''This function specifically computes the gradient of the loss function with respect to the weights and biases in the network.\n",
    "        Inputs:\n",
    "        - predicted_posteriors: It's a 2D array with shape (batch_size, n_classes).[predicted probabilities for each class for each instance in the batch.]\n",
    "        - true_classes:One-hot encoded 2d array with shape (batch_size, n_classes).[These are the true labels for each instance in the batch.]\n",
    "        '''\n",
    "        downstream_gradient = predicted_posteriors - true_classes # cross-entropy loss derivative\n",
    "        for layer in reversed(self.layers[:-1]): # Exclude last layer which is OutputLayer\n",
    "            downstream_gradient = layer.backward(downstream_gradient)\n",
    "    # Handle OutputLayer separately\n",
    "        self.layers[-1].backward(predicted_posteriors, true_classes)\n",
    "\n",
    "    # Update function\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        ''' The update function is responsible for performing one step of training, which includes forward propagation, backward propagation, and updating the weights and biases of the network.\n",
    "        Inputs:\n",
    "        - X: A batch of input data.  A numpy array\n",
    "        - Y: The true labels corresponding to the input data X. A numpy array\n",
    "        - learning_rate :This is a hyperparameter that determines the step size when updating the weights during backpropagation.\n",
    "        '''\n",
    "        # Compute the forward pass\n",
    "        posteriors = self.forward(X)\n",
    "        # Perform the backward pass\n",
    "        self.backward(posteriors, Y)\n",
    "        # Update parameters for each layer\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    # Training function\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the MLP using mini-batch gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - x: A numpy array of shape (N, D) giving training data, where N is the number of data points and D is the number of features.\n",
    "        - y: A numpy array of shape (N, C) giving training labels, where C is the number of classes. Labels should be one-hot encoded.\n",
    "        - n_epochs: An integer giving the number of training epochs (how many times the learning algorithm will work through the entire training set).\n",
    "        - batch_size: An integer giving the number of training examples per mini-batch.\n",
    "        - learning_rate: A float giving the learning rate for the optimization.\n",
    "\n",
    "        Returns: No explicit return, but the weights and biases of the MLP are updated internally.\n",
    "        \"\"\"\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # Reorder data for every epoch\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # Create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # Perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)\n",
    "    @staticmethod\n",
    "    def to_one_hot(Y, n_classes):\n",
    "        ''' The to_one_hot function is a utility function that converts class labels into a one-hot encoded representation.\n",
    "        Input:\n",
    "        Y: a 1-D numpy array of size n_samples containing class labels. Each label is an integer between 0 and n_classes-1.\n",
    "        n_classes: the number of unique classes, a scalar integer.\n",
    "        Output:\n",
    "        A 2-D numpy array of shape (n_samples, n_classes). \n",
    "        '''\n",
    "        n_samples = len(Y)\n",
    "        Y_one_hot = np.zeros((n_samples, n_classes))\n",
    "        Y_one_hot[np.arange(n_samples), Y] = 1\n",
    "        return Y_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
