{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import abstractmethod\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "\n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features  = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None: # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "\n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None: # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        node.threshold = t_min\n",
    "        node.split_index = j_min\n",
    "\n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "\n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        return np.random.default_rng().choice(D, D_try, replace=False)\n",
    "\n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        feature_values = node.features[:, j]\n",
    "        return (feature_values[1:] + feature_values[:-1]) / 2\n",
    "\n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "\n",
    "        left_children = node.features[:, j] <= t\n",
    "        left.features = node.features[left_children]\n",
    "        left.responses = node.responses[left_children]\n",
    "\n",
    "        right.features = node.features[~left_children]\n",
    "        right.responses = node.responses[~left_children]\n",
    "\n",
    "        return left, right\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree\n",
    "To make the differentiation between a ClassificationTree and RegressionTree meaningful, we decided to use the mean of all responses as prediction for leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        left_children = node.features[:, j] <= t\n",
    "\n",
    "        left_responses = node.responses[left_children]\n",
    "        right_responses = node.responses[~left_children]\n",
    "\n",
    "        if len(left_responses) < self.n_min or len(right_responses) < self.n_min:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        left_prediction = np.mean(left_responses)\n",
    "        right_prediction = np.mean(right_responses)\n",
    "\n",
    "        return np.sum((left_responses - left_prediction) ** 2) + np.sum((right_responses - right_prediction) ** 2)\n",
    "\n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        node.prediction = np.mean(node.responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree\n",
    "We decided to use Gini Impurity as loss method, since it already perfectly supports multi-class trees out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "\n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        left_children = node.features[:, j] <= t\n",
    "\n",
    "        left_responses = node.responses[left_children]\n",
    "        right_responses = node.responses[~left_children]\n",
    "\n",
    "        if len(left_responses) < self.n_min or len(right_responses) < self.n_min:\n",
    "            return float(\"inf\")\n",
    "        \n",
    "        _, left_label_counts = np.unique(left_responses, return_counts=True)\n",
    "        _, right_label_counts = np.unique(right_responses, return_counts=True)\n",
    "        \n",
    "        left_impurity = 1 - np.sum((left_label_counts / len(left_responses)) ** 2)\n",
    "        right_impurity = 1 - np.sum((right_label_counts / len(right_responses)) ** 2)\n",
    "\n",
    "        return left_impurity * len(left_responses) + right_impurity * len(right_responses)\n",
    "\n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        node.prediction = scipy.stats.mode(node.responses, axis=None)[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores for classes [3, 9] with n_min = 1 \n",
      " [4.         8.33333333]\n",
      "Average scores for classes [3, 9] with n_min = 2 \n",
      " [3.83333333 8.16666667]\n",
      "Average scores for classes [3, 9] with n_min = 5 \n",
      " [4.81388889 8.04166667]\n",
      "Average scores for classes [3, 9] with n_min = 10 \n",
      " [3.47619048 8.27777778]\n",
      "Average scores for classes [3, 9] with n_min = 20 \n",
      " [4.53015873 7.75238095]\n",
      "Average scores for classes [3, 9] with n_min = 50 \n",
      " [4.53842242 7.87329109]\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "\n",
    "splitter = KFold(n_splits=10)\n",
    "n_mins = [1, 2, 5, 10, 20, 50]\n",
    "classes = list(np.unique(labels))\n",
    "grid = np.linspace(-10, 10, 100)\n",
    "\n",
    "for n_min in n_mins:\n",
    "    tree = RegressionTree(n_min=n_min)\n",
    "\n",
    "    for train_index, test_index in splitter.split(features):\n",
    "        tree.train(features[train_index], labels[train_index])\n",
    "        deviations = np.zeros((len(classes), 2))\n",
    "\n",
    "        for sample in test_index:\n",
    "            label = labels[sample]\n",
    "            prediction = tree.predict(features[sample])\n",
    "            deviations[classes.index(label)] += [prediction, 1]\n",
    "\n",
    "    print(\"Average scores for classes\", classes, \"with n_min =\", n_min, \"\\n\", deviations[:, 0] / deviations[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using ClassificationTree(classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "\n",
    "splitter = KFold(n_splits=10)\n",
    "n_mins = [1, 2, 5, 10, 20, 50]\n",
    "classes = list(np.unique(labels))\n",
    "\n",
    "for n_min in n_mins:\n",
    "    tree = ClassificationTree(classes=classes, n_min=n_min)\n",
    "    confusion_matrix = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "    for train_index, test_index in splitter.split(features):\n",
    "        tree.train(features[train_index], labels[train_index])\n",
    "\n",
    "        for sample in test_index:\n",
    "            label = labels[sample]\n",
    "            prediction = tree.predict(features[sample])\n",
    "            confusion_matrix[classes.index(label)][classes.index(prediction)] += 1\n",
    "\n",
    "    print(\"Confusion matrix for classes\", classes, \"with n_min =\", n_min, \"\\n\", confusion_matrix)\n",
    "    print(\"Accuracy: {:.1%}\".format(np.trace(confusion_matrix) / np.sum(confusion_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our training step took us less than one second per training set, and the variance between different runs caused by the random feature selection was too high to get meaningful results out of the cross-validation, we increased the number of splits to 10 and calculated the confusion matrices for different `n_min` values, all the way from 1 to 50.\n",
    "\n",
    "Surprisingly, the impact of reducing `n_min` further below a threshold of approximately 10-20 does not seem to have any significant impact on the prediction quality, while still increasing the training duration (Due to the higher depth of the resulting tree). Overall, a single ClassificationTree was only able to reach an accuracy of 85-90%.\n",
    "\n",
    "## Proof of Concept: Multi-Class Classification\n",
    "Our current implementations of RegressionTree and ClassificationTree support multiple classes:\n",
    "\n",
    "RegressionTree can handle any score (Not just -1 and 1), as already shown in the cross-valdation.\n",
    "To demonstrate the multi-class functionality of ClassificationTree, we will run a simple test case with three instead of two labels.\n",
    "\n",
    "Technically speaking, we even ignore the supplies `classes` parameter, and only carry it for compatibility purposes.\n",
    "The actually returned label in ClassificationTree is generated directly from the `reponses` supplied during training, which is why the number of labels don't matter to our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for classes [3, 7, 9] \n",
      " [[155.   6.  22.]\n",
      " [  8. 148.  23.]\n",
      " [ 31.  13. 136.]]\n",
      "Accuracy: 81.0%\n"
     ]
    }
   ],
   "source": [
    "instances = (digits.target == 3) | (digits.target == 7) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "splitter = KFold(n_splits=10)\n",
    "classes = list(np.unique(labels))\n",
    "\n",
    "tree = ClassificationTree(classes=classes)\n",
    "confusion_matrix = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "for train_index, test_index in splitter.split(features):\n",
    "    tree.train(features[train_index], labels[train_index])\n",
    "\n",
    "    for sample in test_index:\n",
    "        label = labels[sample]\n",
    "        prediction = tree.predict(features[sample])\n",
    "        confusion_matrix[classes.index(label)][classes.index(prediction)] += 1\n",
    "\n",
    "print(\"Confusion matrix for classes\", classes, \"\\n\", confusion_matrix)\n",
    "print(\"Accuracy: {:.1%}\".format(np.trace(confusion_matrix) / np.sum(confusion_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(features, responses):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "    ... # your code here\n",
    "    raise NotImplementedError(\"bootstrap_sampling(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees, n_min=10):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree(n_min) for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        ... # your code here\n",
    "        raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationForest():\n",
    "    def __init__(self, n_trees, classes, n_min=1):\n",
    "        self.trees = [ClassificationTree(classes, n_min) for i in range(n_trees)]\n",
    "        self.classes = classes\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        ... # your code here\n",
    "        raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionForest(n_trees=10)\n",
    "# and comment on your results\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using DecisionForest(n_trees=10, classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DecisionForest(n_trees=10, classes=np.unique(digits.target))\n",
    "# for all 10 digits simultaneously.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-against-the-rest classification with RegressionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ten one-against-the-rest regression forests for the 10 digits.\n",
    "# Make sure that all training sets are balanced between the current digit and the rest.\n",
    "# Assign test instances to the digit with highest score, \n",
    "# or to \"unknown\" if all scores are negative.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "... # your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
